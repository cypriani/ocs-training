<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Red Hat Ceph Storage Stretch Cluster With Arbiter Deployment :: OCS Training</title>
    <link rel="canonical" href="https://red-hat-storage.github.io/ocs-training/training/ocs4/rhcs-stretched-deploy.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LGCEEZGN54"></script>
    <script>function gtag(){dataLayer.push(arguments)};window.dataLayer=window.dataLayer||[];gtag('js',new Date());gtag('config','G-LGCEEZGN54')</script>
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="40px" alt="Red Hat Data Services">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage" target="_blank">OCS Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/blob/master/CONTRIBUTING.adoc" target="_blank">Guidelines</a>
            <a class="navbar-item" href="https://github.com/red-hat-storage/ocs-training/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">OCS Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">OCS Installation and Configuration</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="odf.html">ODF General deploy and use</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-install-no-ui.html">OCS CLI based install</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="odf4-install-no-ui.html">ODF CLI based install</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-encryption.html">External KMS Encryption</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-enable-rgw.html">Use RGW in OCS deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-cluster-storage-quotas.html">Cluster wide storage management</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Disaster recovery</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="odf4-multisite-ramen.html">ODF 4.9 Regional disaster recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="odf410-multisite-ramen.html">ODF 4.10 Regional disaster recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-metro-stretched.html">Stretch Cluster disaster recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="odf4-metro-ramen.html">ODF 4.10 Metro disaster recovery</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Development preview features</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-additionalfeatures-dbwal.html">BlueStore RocksDB metadata and WAL placement</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-additionalfeatures-devtype.html">Mixed OSD device type configuration</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-additionalfeatures-override.html">Ceph configuration override</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ocs4-additionalfeatures-segregation.html">Data Segregation</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../infra-nodes/ocs4-infra-nodes.html">Deploying on Infra nodes</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../ocs4perf/ocs4perf.html">Test deployment post-install</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">OCS Installation and Configuration</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">OCS Installation and Configuration</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../../RegionalDR/index.html">ODF Regional DR</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../../RegionalDR/index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">OCS Installation and Configuration</a></li>
    <li><a href="rhcs-stretched-deploy.html">Red Hat Ceph Storage Stretch Cluster With Arbiter Deployment</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="https://github.com/red-hat-storage/ocs-training/edit/master/training/modules/ocs4/pages/rhcs-stretched-deploy.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Red Hat Ceph Storage Stretch Cluster With Arbiter Deployment</h1>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_overview">1. Overview</a></li>
<li><a href="#_rhcs_stretch_mode_introduction">2. RHCS stretch mode introduction</a></li>
<li><a href="#_requirements_for_deploying_red_hat_ceph_storage_stretch_cluster_with_arbiter">3. Requirements for deploying Red Hat Ceph Storage stretch cluster with arbiter</a>
<ul class="sectlevel2">
<li><a href="#_hardware_requirements">3.1. Hardware requirements</a></li>
<li><a href="#_network_configurations">3.2. Network Configurations</a></li>
</ul>
</li>
<li><a href="#_node_pre_deployment_requirements">4. Node Pre-Deployment Requirements</a>
<ul class="sectlevel2">
<li><a href="#_repositories_and_packages">4.1. Repositories and packages</a></li>
</ul>
</li>
<li><a href="#_cluster_bootstrapping_with_cephadm">5. Cluster Bootstrapping with Cephadm</a></li>
<li><a href="#_relocating_the_monitors_and_managers_to_an_appropriate_location">6. Relocating the monitors and managers to an appropriate location</a></li>
<li><a href="#_deploying_ceph_services">7. Deploying Ceph services</a></li>
<li><a href="#_configuring_red_hat_ceph_storage_stretch_cluster">8. Configuring Red Hat Ceph Storage stretch cluster</a></li>
</ul>
</div>
<div class="sect1">
<h2 id="_overview"><a class="anchor" href="#_overview"></a>1. Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat Ceph Storage (RHCS) is an enterprise open-source platform that provides unified software-defined storage on standard, economical servers and disks. With block, object, and file storage combined into one platform, Red Hat Ceph Storage efficiently and automatically manages all your data, so you can focus on the applications and workloads that use it.</p>
</div>
<div class="paragraph">
<p>In this guide, we will explain how to properly set up a Red Hat Ceph Storage 5 cluster deployed on two different datacenters using the stretched mode functionality.</p>
</div>
<div class="paragraph">
<p>Also, RHCS provides other advanced characteristics like:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Decouples software from hardware to run cost-effectively on industry-standard servers and disks.</p>
</li>
<li>
<p>Scales flexibly and massively to support multiple petabyte deployments with consistent performance.</p>
</li>
<li>
<p>Provides web-scale object storage for modern use cases, such as cloud infrastructure, media repository, and big data analytics.</p>
</li>
<li>
<p>Combines the most stable version of Ceph with a storage management console, deployment tools, and support services.</p>
</li>
<li>
<p>Object, block, and file storage.</p>
</li>
<li>
<p>Compatibility with Amazon S3 object application programming interface (API), OpenStack Swift, NFS v4, or native API protocols.</p>
</li>
<li>
<p>Block storage integrated with OpenStack, Linux, and KVM hypervisor.</p>
</li>
<li>
<p>Validated with Apache Hadoop S3A filesystem client.</p>
</li>
<li>
<p>Multi-site and disaster recovery options.</p>
</li>
<li>
<p>Flexible storage policies.</p>
</li>
<li>
<p>Data durability via erasure coding or replication.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the diagram depicted below, we can see a graphical representation of the RHCS
architecture that will be used in this guide:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/RHCS-stretch-cluster-arbiter.png" alt="High Level Architecture RHCS stretch mode">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_rhcs_stretch_mode_introduction"><a class="anchor" href="#_rhcs_stretch_mode_introduction"></a>2. RHCS stretch mode introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When the stretch mode is enabled, the OSDs will only take PGs active when they peer across datacenters, assuming both are alive with the following constraints:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Pools will increase in size from the default 3 to 4, expecting two copies on each site.</p>
</li>
<li>
<p>OSDs will only be allowed to connect to monitors in the same data center.</p>
</li>
<li>
<p>New monitors will not join the cluster if they do not specify a location.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If all the OSDs and monitors from a datacenter become inaccessible at once, the surviving datacenter will enter a degraded stretch mode which implies:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>This will issue a warning, reduce the pool&#8217;s <code>min_size</code> to 1, and allow the cluster to go active with data in the single remaining site.</p>
</li>
<li>
<p>The pool <code>size</code> parameter is not changed, so you will also get warnings that the pools are too small.</p>
</li>
<li>
<p>Although, the stretch mode flag will prevent the OSDs from creating extra copies in the remaining datacenter (so it will only keep two copies, as before).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When the missing data center comes back, the cluster will enter recovery stretch mode triggering the following actions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>This changes the warning and allows peering, but still only requires OSDs from the datacenter, which was up the whole time.</p>
</li>
<li>
<p>When all PGs are in a known state and are neither degraded nor incomplete, the cluster transitions back to the regular stretch mode where:</p>
<div class="ulist">
<ul>
<li>
<p>The cluster ends the warning.</p>
</li>
<li>
<p>Restores <code>min_size</code> to its starting value (2) and requires both sites to peer.</p>
</li>
<li>
<p>Stops requiring the always-alive site when peering (so that you can failover to the other site, if necessary).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_requirements_for_deploying_red_hat_ceph_storage_stretch_cluster_with_arbiter"><a class="anchor" href="#_requirements_for_deploying_red_hat_ceph_storage_stretch_cluster_with_arbiter"></a>3. Requirements for deploying Red Hat Ceph Storage stretch cluster with arbiter</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat Ceph Storage (RHCS) is an enterprise open-source platform that provides unified software-defined storage on standard, economical servers and disks. With block, object, and file storage combined into one platform, Red Hat Ceph Storage efficiently and automatically manages all your data, so you can focus on the applications and workloads that use it.</p>
</div>
<div class="paragraph">
<p>This section provides a basic overview of the RHCS deployment. For more complex
deployment, refer to the <a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5">official documentation guide for RHCS 5.</a></p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Only Flash media is supported since it runs with <code>min_size=1</code> when degraded. Use stretch mode only with all-flash OSDs. Using all-flash OSDs minimizes the time needed to recover once connectivity is restored, thus minimizing the potential for data loss. Erasure coded pools cannot be used with stretch mode.
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Erasure coded pools cannot be used with stretch mode.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Custom CRUSH rules providing two copies in each site (using a total of 4 copies) must be created when configuring the stretch mode in the Ceph cluster.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_hardware_requirements"><a class="anchor" href="#_hardware_requirements"></a>3.1. Hardware requirements</h3>
<div class="paragraph">
<p>For information on minimum hardware requirements for deploying Red Hat Ceph
Storage, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html-single/hardware_guide/index#minimum-hardware-recommendations-for-containerized-ceph_hwMinimum">hardware recommendations for containerized Ceph.</a></p>
</div>
<div class="paragraph">
<p>Physical server locations and Ceph component layout for Red Hat Ceph Storage cluster deployment.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-center valign-top">Node name</th>
<th class="tableblock halign-center valign-top">Datacenter</th>
<th class="tableblock halign-center valign-top">Ceph components</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">ceph1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">DC1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">OSD+MON</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">ceph2</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">DC1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">OSD+MON+MGR</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">ceph3</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">DC1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">OSD+MDS+RGW</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">ceph4</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">DC2</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">OSD+MON</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">ceph5</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">DC2</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">OSD+MON+MGR</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">ceph6</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">DC2</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">OSD+MDS+RGW</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">ceph7</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">DC3</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">MON</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Software Details:</p>
</div>
<div class="paragraph">
<p>Use the latest software version of RHCS 5. See
<a href="https://access.redhat.com/articles/1548993">the knowledgebase article on
Red Hat Ceph Storage: Supported configurations.</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_network_configurations"><a class="anchor" href="#_network_configurations"></a>3.2. Network Configurations</h3>
<div class="paragraph">
<p>The recommended Red Hat Ceph Storage configuration are as follows:
You must have two separate networks, one public network and one private network.
You must have three different datacenters with L2 or L3 connectivity between all the nodes that form the Ceph cluster.
NOTE: You can use different subnets for each of the datacenters.</p>
</div>
<div class="paragraph">
<p>Here is an example of a basic network configuration that we have used in this guide:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>DC1:</strong> <strong>Ceph public/private network:</strong> 10.40.0.0/24</p>
</li>
<li>
<p><strong>DC2:</strong> <strong>Ceph public/private network:</strong> 10.40.0.0/24</p>
</li>
<li>
<p><strong>DC3:</strong> <strong>Ceph public/private network:</strong> 10.40.0.0/24</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information on the required network environment, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/configuration_guide/ceph-network-configuration">Ceph
network configuration.</a></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_node_pre_deployment_requirements"><a class="anchor" href="#_node_pre_deployment_requirements"></a>4. Node Pre-Deployment Requirements</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before installing the RHCS Ceph cluster we need to perform the following steps in order to fulfil all the requirements needed:</p>
</div>
<div class="sect2">
<h3 id="_repositories_and_packages"><a class="anchor" href="#_repositories_and_packages"></a>4.1. Repositories and packages</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Register all the nodes to the Red Hat Network or Red Hat Satellite and subscribe to a valid pool:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">subscription-manager register
subscription-manager subscribe --pool=8a8XXXXXX9e0</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example you can use <code>ceph1</code> as the deployment node. On ceph1, run the cephadm preflight ansible playbooks. For this you will need ansible 2.9 repos enabled in ceph1.</p>
</div>
</li>
<li>
<p>Enable the following repositories:</p>
<div class="ulist">
<ul>
<li>
<p><code>rhel-8-for-x86_64-baseos-rpms</code></p>
</li>
<li>
<p><code>rhel-8-for-x86_64-appstream-rpms</code></p>
</li>
<li>
<p><code>rhceph-5-tools-for-rhel-8-x86_64-rpms</code></p>
</li>
<li>
<p><code>ansible-2.9-for-rhel-8-x86_64-rpms</code> (only in the <code>ceph1</code> host)</p>
</li>
</ul>
</div>
</li>
<li>
<p>Enable the repos on all the servers that are going to be part of the RCHS cluster</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">subscription-manager repos --disable="*" --enable="rhel-8-for-x86_64-baseos-rpms" --enable="rhel-8-for-x86_64-appstream-rpms" --enable="rhceph-5-tools-for-rhel-8-x86_64-rpms"</code></pre>
</div>
</div>
</li>
<li>
<p>On the <code>ceph1</code> host also enable the <code>ansible-2.9-for-rhel-8-x86_64-rpms</code> repository:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">subscription-manager repos --enable="ansible-2.9-for-rhel-8-x86_64-rpms"</code></pre>
</div>
</div>
</li>
<li>
<p>Update the system rpms to the latest version and reboot if needed:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">dnf update -y
reboot</code></pre>
</div>
</div>
</li>
<li>
<p>In all our hosts we configure the hostname using the bare/short hostname.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">hostnamectl set-hostname &lt;short_name&gt;</code></pre>
</div>
</div>
</li>
<li>
<p>Modify /etc/hosts file and add the fqdn entry to the 127.0.0.1 IP by setting the DOMAIN variable with our DNS domain name.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">DOMAIN="bkgzv.sandbox762.opentlc.com"
cat &lt;&lt;EOF &gt;/etc/hosts
127.0.0.1 $(hostname).${DOMAIN} $(hostname) localhost localhost.localdomain localhost4 localhost4.localdomain4
::1       $(hostname).${DOMAIN} $(hostname) localhost6 localhost6.localdomain6
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Verify the configuration for deploying Red Hat Ceph Storage with cephadm.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">hostname</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>ceph1</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Check the long hostname with the fqdn using the hostname -f option.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">hostname -f</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>ceph1.bkgzv.sandbox762.opentlc.com</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Install the <code>cephadm-ansible</code> RPM package:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">sudo dnf install -y cephadm-ansible</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
To run the ansible playbooks, you must have ssh passwordless access to all the nodes that are configured to the Red Hat Ceph Storage cluster. Ensure that the configured user (for example, ec2-user) has root privileges to use the <code>sudo</code> command.
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Configure the ec2-user ssh config file to specify the user and id/key that can be used for connecting to the nodes via ssh:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cat &lt;&lt;EOF &gt; ~/.ssh/config
Host ceph*
   User ec2-user
   IdentityFile ~/.ssh/ceph.pem
EOF</code></pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Build our ansible inventory</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cat &lt;&lt;EOF &gt; /usr/share/cephadm-ansible/inventory
ceph1
ceph2
ceph3
ceph4
ceph5
ceph6
ceph7
[admin]
ceph1
EOF</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>cephadm-ansible</code> pre-flight playbook deploys the RHCS admin keyring to all hosts belonging to the [admin] group at the following location /etc/ceph/ceph.client.admin.keyring.
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Verify ansible can access all of the nodes using the ping module before running the pre-flight playbook.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ansible -i /usr/share/cephadm-ansible/inventory -m ping all -b</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>ceph6 | SUCCESS =&gt; {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/libexec/platform-python"
    },
    "changed": false,
    "ping": "pong"
}
ceph4 | SUCCESS =&gt; {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/libexec/platform-python"
    },
    "changed": false,
    "ping": "pong"
}
ceph3 | SUCCESS =&gt; {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/libexec/platform-python"
    },
    "changed": false,
    "ping": "pong"
}
ceph2 | SUCCESS =&gt; {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/libexec/platform-python"
    },
    "changed": false,
    "ping": "pong"
}
ceph5 | SUCCESS =&gt; {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/libexec/platform-python"
    },
    "changed": false,
    "ping": "pong"
}
ceph1 | SUCCESS =&gt; {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/libexec/platform-python"
    },
    "changed": false,
    "ping": "pong"
}
ceph7 | SUCCESS =&gt; {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/libexec/platform-python"
    },
    "changed": false,
    "ping": "pong"
}</pre>
</div>
</div>
<div class="paragraph">
<p>The preflight Ansible playbook configures the Ceph repository and prepares the storage cluster for bootstrapping. It also installs some prerequisites, such as podman, lvm2, chronyd, and cephadm. The default location for cephadm-ansible and cephadm-preflight.yml is /usr/share/cephadm-ansible.</p>
</div>
</div>
</div>
</li>
<li>
<p>Run the following ansible playbook.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ansible-playbook -i /usr/share/cephadm-ansible/inventory /usr/share/cephadm-ansible/cephadm-preflight.yml --extra-vars "ceph_origin=rhcs"</code></pre>
</div>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cluster_bootstrapping_with_cephadm"><a class="anchor" href="#_cluster_bootstrapping_with_cephadm"></a>5. Cluster Bootstrapping with Cephadm</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The cephadm utility installs and starts a Ceph Monitor daemon and a Ceph Manager daemon for a new Red Hat Ceph Storage cluster on the local node where the cephadm bootstrap command is run.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For additional information on the bootstrapping process, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/installation_guide/red-hat-ceph-storage-installation#bootstrapping-a-new-storage-cluster_install">Bootstrapping
a new storage cluster.</a>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><code>Procedure</code></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create json file to authenticate against the container registry using a json file as follows:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cat &lt;&lt;EOF &gt; /root/registry.json
{
 "url":"registry.redhat.io",
 "username":"User",
 "password":"Pass"
}
EOF</code></pre>
</div>
</div>
</li>
<li>
<p>Configure Host Specs file.</p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>You can use a service configuration file and the --apply-spec option to bootstrap the storage cluster and configure additional hosts and daemons. The configuration file is a .yaml file that contains the service type, placement, and designated nodes for services that you want to deploy.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cat &lt;&lt;EOF &gt; /root/cluster-spec.yaml
service_type: host
addr: 10.0.40.78  ## &lt;XXX.XXX.XXX.XXX&gt;
hostname: ceph1   ##  &lt;ceph-hostname-1&gt;
---
service_type: host
addr: 10.0.40.35
hostname: ceph2
---
service_type: host
addr: 10.0.40.24
hostname: ceph3
---
service_type: host
addr: 10.0.40.185
hostname: ceph4
---
service_type: host
addr: 10.0.40.88
hostname: ceph5
---
service_type: host
addr: 10.0.40.66
hostname: ceph6
---
service_type: host
addr: 10.0.40.221
hostname: ceph7
EOF</code></pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Retrieve the IP for the NIC that has the RHCS public network configured from the bootstrap node. You can use the following example command after substituting the <code>10.0.40.0</code> with the subnet you have defined in your ceph public network.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ip a | grep 10.0.40</code></pre>
</div>
</div>
<div class="paragraph">
<p>Example output:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>10.0.40.78</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Run the <code>Cephadm</code> bootstrap command as the root user on the node that will be the initial Monitor node in the cluster. The <code>IP_ADDRESS</code> option is the IP address of the node that you are using to run the <code>cephadm bootstrap</code> command.</p>
<div class="openblock">
<div class="content">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you have configured a different user than root for passwordless SSH access use the <code>--ssh-user=</code> flag with the cepadm bootstrap command
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">cephadm  bootstrap --ssh-user=ec2-user --mon-ip 10.0.40.78 --apply-spec /root/cluster-spec.yaml --registry-json /root/registry.json</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
If the local node uses fully-qualified domain names (FQDN), then add the <code>--allow-fqdn-hostname</code> option to cephadm bootstrap on the command line.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Once the bootstrap finishes, you will see the following output from the previous cephadm bootstrap command:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">You can access the Ceph CLI with:

	sudo /usr/sbin/cephadm shell --fsid dd77f050-9afe-11ec-a56c-029f8148ea14 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/</code></pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Verify the status of Red Hat Ceph Storage cluster deployment using the ceph cli client from ceph1:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph -s</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>  cluster:
    id:     dd77f050-9afe-11ec-a56c-029f8148ea14
    health: HEALTH_WARN
            OSD count 0 &lt; osd_pool_default_size 3

  services:
    mon: 5 daemons, quorum ceph1,ceph4,ceph6,ceph3,ceph5 (age 2m)
    mgr: ceph1.laagvc(active, since 6m), standbys: ceph4.adlrnk
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:</pre>
</div>
</div>
<div class="paragraph">
<p>You now have 5 monitors running (this is now the default with cephadm deployments) if enough nodes are available. The cluster is in HEALTH_WARN state since no OSDs are deployed as yet.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
It is normal if you get a global recovery event while you don&#8217;t have any osds configured.
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Check if all the services have started.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch ls</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>NAME           RUNNING  REFRESHED  AGE  PLACEMENT
alertmanager       1/1  52s ago    6m   count:1
crash              7/7  2m ago     7m   *
grafana            1/1  52s ago    6m   count:1
mgr                2/2  54s ago    7m   count:2
mon                5/5  118s ago   7m   count:5
node-exporter      7/7  2m ago     6m   *
prometheus         1/1  52s ago    6m   count:1</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
It may take a while for all the services to start.
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Verify if all the nodes are part of the cephadm cluster.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch host ls</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>HOST   ADDR          LABELS  STATUS
ceph1  10.0.40.78
ceph2  10.0.40.35
ceph3  10.0.40.24
ceph4  10.0.40.185
ceph5  10.0.40.88
ceph6  10.0.40.66
ceph7  10.0.40.221</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
We can run direct ceph commands from the host because we configured ceph1
in the cephadm-ansible inventory as part of the [admin] group, so the ceph
admin keys were copied to the host during the cephadm bootstrap process
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_relocating_the_monitors_and_managers_to_an_appropriate_location"><a class="anchor" href="#_relocating_the_monitors_and_managers_to_an_appropriate_location"></a>6. Relocating the monitors and managers to an appropriate location</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat Ceph Storage with <code>cephadm</code> deploys five monitors by default.  If they are not placed according to the topology architecture, use this procedure to correctly configure the ceph monitors to the appropriate datacenters.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check the current placement of the Ceph monitor services on the datacenters.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch ps | grep mon | awk '{print $1 " " $2}'</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>mon.ceph1 ceph1
mon.ceph2 ceph2
mon.ceph4 ceph4
mon.ceph5 ceph5
mon.ceph6 ceph6</pre>
</div>
</div>
<div class="paragraph">
<p>You can see that there are 2 monitors on nodes in datacenter1, and three  monitors on nodes in datacenter2 but no monitors in datacenter3. Refer to the table in the Hardware requirements section to verify the layout of the ceph servers per datacenter.</p>
</div>
</div>
</div>
</li>
<li>
<p>Move a monitor to the node <code>ceph7</code> located in our datacenter3 site.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch apply mon --placement="ceph1,ceph2,ceph4,ceph5,ceph7"</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>Scheduled mon update...</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Verify that the monitors are in the correct layout:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch ps | grep mon | awk '{print $1 " " $2}'</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>mon.ceph1 ceph1
mon.ceph2 ceph2
mon.ceph4 ceph4
mon.ceph5 ceph5
mon.ceph7 ceph7</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Check the current placement of the Ceph manager services on the datacenters.</p>
<div class="openblock">
<div class="content">
<div class="literalblock">
<div class="content">
<pre>ceph orch ps | grep mgr | awk '{print $1 " " $2}'</pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output:</div>
<div class="content">
<pre>mgr.ceph1.ycgwy ceph1
mgr.ceph2.kremtt ceph2</pre>
</div>
</div>
<div class="paragraph">
<p>You can see that there are 2 managers on nodes datacenter1, and no managers in datacenter2. Refer to the table in the Hardware requirements section to verify the layout of the ceph servers per datacenter.</p>
</div>
</div>
</div>
</li>
<li>
<p>Move a manager to the node <code>ceph5</code> located in our datacenter2 site.</p>
<div class="openblock">
<div class="content">
<div class="literalblock">
<div class="content">
<pre>ceph orch apply mgr --placement="ceph2,ceph5"</pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output:</div>
<div class="content">
<pre>Scheduled mgr update…</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Verify that the managers are in the correct layout:</p>
<div class="openblock">
<div class="content">
<div class="literalblock">
<div class="content">
<pre>ceph orch ps | grep mgr | awk '{print $1 " " $2}'</pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output:</div>
<div class="content">
<pre>mgr.ceph2.ycgwyz ceph2
mgr.ceph5.kremtt ceph5</pre>
</div>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploying_ceph_services"><a class="anchor" href="#_deploying_ceph_services"></a>7. Deploying Ceph services</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section provides examples of how you can deploy the following Ceph services:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Object Storage Devices (OSDs)</code></p>
</li>
<li>
<p><code>Metadata services (MDS) required for CephFS</code></p>
</li>
<li>
<p><code>Rados Gateway Services(RGW) required for object storage</code></p>
</li>
<li>
<p><code>Rados Block Device (RBD) pool required for block storage</code></p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
it is mandatory to deploy the OSDs for the cluster to work.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><code>Procedure</code></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Scan the hosts to check for available disks and to get the OSD layout. In this example, 6 OSDs will be created one per each ceph hosts where <code>ceph7</code> is the arbiter node which does not have any OSD&#8217;s configured.</p>
<div class="openblock">
<div class="content">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Use the --all-available-devices option in the command if you wish to use all the free disks in the ceph nodes. Run this command twice; the first time to scan the nodes, and the second time to view the results. For details, see Management of services using the Ceph Orchestrator.
</td>
</tr>
</table>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch apply osd --all-available-devices --dry-run</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>####################
SERVICESPEC PREVIEWS
####################
+---------+------+--------+-------------+
|SERVICE  |NAME  |ADD_TO  |REMOVE_FROM  |
+---------+------+--------+-------------+
+---------+------+--------+-------------+
################
OSDSPEC PREVIEWS
################</pre>
</div>
</div>
<div class="paragraph">
<p>If we re-run the same command after a minute we can see that the devices on the
nodes have been discovered, and are available to be used as OSDs.</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch apply osd --all-available-devices --dry-run</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>####################
SERVICESPEC PREVIEWS
####################
+---------+------+--------+-------------+
|SERVICE  |NAME  |ADD_TO  |REMOVE_FROM  |
+---------+------+--------+-------------+
+---------+------+--------+-------------+
################
OSDSPEC PREVIEWS
################
+---------+-----------------------+-------+-----------+----+-----+
|SERVICE  |NAME                   |HOST   |DATA       |DB  |WAL  |
+---------+-----------------------+-------+-----------+----+-----+
|osd      |all-available-devices  |ceph1  |/dev/xvdh  |-   |-    |
|osd      |all-available-devices  |ceph2  |/dev/xvdh  |-   |-    |
|osd      |all-available-devices  |ceph3  |/dev/xvdh  |-   |-    |
|osd      |all-available-devices  |ceph4  |/dev/xvdh  |-   |-    |
|osd      |all-available-devices  |ceph5  |/dev/xvdh  |-   |-    |
|osd      |all-available-devices  |ceph6  |/dev/xvdh  |-   |-    |
+---------+-----------------------+-------+-----------+----+-----+</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Run the command without –dry-run to create the OSD services</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch apply osd --all-available-devices</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>Scheduled osd.all-available-devices update...</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Check the ceph osd crush map layout to ensure that each host has one OSD configured and its status is UP.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd tree</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.87900  root default
-11         0.14650      host ceph1
  2    ssd  0.14650          osd.2       up   1.00000  1.00000
 -3         0.14650      host ceph2
  3    ssd  0.14650          osd.3       up   1.00000  1.00000
-13         0.14650      host ceph3
  4    ssd  0.14650          osd.4       up   1.00000  1.00000
 -5         0.14650      host ceph4
  0    ssd  0.14650          osd.0       up   1.00000  1.00000
 -9         0.14650      host ceph5
  1    ssd  0.14650          osd.1       up   1.00000  1.00000
 -7         0.14650      host ceph6
  5    ssd  0.14650          osd.5       up   1.00000  1.00000</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Create and enable RBD a new block pool.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd pool create rbdpool 32 32
ceph osd pool application enable rbdpool rbd</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The number 32 at the end of the command is the number of PGs assigned to this pool, the number of PGs can vary depending on several factors like the number of OSDs in the cluster, expected % used of the pool, etc. You can use the following calculator to help you determine the number of PGs needed: <a href="https://access.redhat.com/labs/cephpgc/" class="bare">https://access.redhat.com/labs/cephpgc/</a>
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Verify that the RBD pool has been created.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd lspools | grep rbdpool</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>3 rbdpool</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Using cephadm, deploy two new MDS daemons one in each datacenter. For more information, see <a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/operations_guide/management-of-mds-service-using-the-ceph-orchestrator">Management of MDS service using the Ceph Orchestrator.</a></p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>In our example, we are using hosts <code>ceph3</code> and <code>ceph6</code>:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch apply mds cephfs --placement=ceph3,ceph6</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>Scheduled mds.cephfs update...</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Verify that MDS services are active.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph -s | grep mds</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>    mds: 1/1 daemons up, 1 standby</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Create the CephFS volume. For placements, use the same <code>hostname</code> as used in the previous command.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph fs volume create cephfs --placement=ceph3,ceph6</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The ceph fs volume create command also creates the needed data and meta
CephFS pools. For more information, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/file_system_guide/index">Configuring
and Mounting Ceph File Systems.</a>
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Check the Ceph status to verify how the MDS daemons have been deployed. Ensure that the state is active where <code>ceph6</code> is the primary MDS for this filesystem and ceph3 is the secondary MDS.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph fs status</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>cephfs - 0 clients
======
RANK  STATE           MDS             ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  cephfs.ceph6.ggjywj  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL
cephfs.cephfs.meta  metadata  96.0k   284G
cephfs.cephfs.data    data       0    284G
    STANDBY MDS
cephfs.ceph3.ogcqkl</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Deploy the object services such that there is one RGW service per OSD datacenter.
Use the --placement argument as shown in below command. The initial number 2 in the example represents the number of RGW daemons to deploy, followed by the placement for the daemons specifying the hostname where the daemon must run. In this example, the hostname’s are ceph3 and ceph6.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch apply rgw objectgw  --port=8080 --placement="2 ceph3 ceph6"</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>Scheduled rgw.objectgw update...</pre>
</div>
</div>
<div class="paragraph">
<p>To know more about RGW, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html-single/object_gateway_guide/index">The
Ceph Object Gateway.</a></p>
</div>
</div>
</div>
</li>
<li>
<p>Verify that RGW services are active.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph -s | grep rgw</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>rgw: 2 daemons active (2 hosts, 1 zones)</pre>
</div>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_configuring_red_hat_ceph_storage_stretch_cluster"><a class="anchor" href="#_configuring_red_hat_ceph_storage_stretch_cluster"></a>8. Configuring Red Hat Ceph Storage stretch cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p><code>Procedure</code></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check the current election strategy being used by the monitors with the ceph mon dump command. By default in a ceph cluster, the connectivity is set to classic.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ ceph mon dump | grep election_strategy</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>dumped monmap epoch 9
election_strategy: 1</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Change the monitor election to connectivity.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph mon set election_strategy connectivity</code></pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Run the previous ceph mon dump command again to verify the election_strategy value.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ ceph mon dump | grep election_strategy</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>dumped monmap epoch 10
election_strategy: 3</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
To know more about the different election strategies, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/operations_guide/management-of-monitors-using-the-ceph-orchestrator#configuring-monitor-election-strategy_ops">Configuring
monitor election strategy.</a>
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Set the location for all our Ceph monitors:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph mon set_location ceph1 datacenter=DC1
ceph mon set_location ceph2 datacenter=DC1
ceph mon set_location ceph4 datacenter=DC2
ceph mon set_location ceph5 datacenter=DC2
ceph mon set_location ceph7 datacenter=DC3</code></pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Verify that each monitor has its appropriate location.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph mon dump</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>epoch 17
fsid dd77f050-9afe-11ec-a56c-029f8148ea14
last_changed 2022-03-04T07:17:26.913330+0000
created 2022-03-03T14:33:22.957190+0000
min_mon_release 16 (pacific)
election_strategy: 3
0: [v2:10.0.143.78:3300/0,v1:10.0.143.78:6789/0] mon.ceph1; crush_location {datacenter=DC1}
1: [v2:10.0.155.185:3300/0,v1:10.0.155.185:6789/0] mon.ceph4; crush_location {datacenter=DC2}
2: [v2:10.0.139.88:3300/0,v1:10.0.139.88:6789/0] mon.ceph5; crush_location {datacenter=DC2}
3: [v2:10.0.150.221:3300/0,v1:10.0.150.221:6789/0] mon.ceph7; crush_location {datacenter=DC3}
4: [v2:10.0.155.35:3300/0,v1:10.0.155.35:6789/0] mon.ceph2; crush_location {datacenter=DC1}</pre>
</div>
</div>
</li>
<li>
<p>View the current CRUSH map</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd tree</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.87900  root default
-11         0.14650      host ceph1
  2    ssd  0.14650          osd.2       up   1.00000  1.00000
 -3         0.14650      host ceph2
  3    ssd  0.14650          osd.3       up   1.00000  1.00000
-13         0.14650      host ceph3
  4    ssd  0.14650          osd.4       up   1.00000  1.00000
 -5         0.14650      host ceph4
  0    ssd  0.14650          osd.0       up   1.00000  1.00000
 -9         0.14650      host ceph5
  1    ssd  0.14650          osd.1       up   1.00000  1.00000
 -7         0.14650      host ceph6
  5    ssd  0.14650          osd.5       up   1.00000  1.00000</pre>
</div>
</div>
<div class="paragraph">
<p>This default crush map indicates that the failure domain is at the host level and ceph has no understanding of what the infrastructure topology looks like.</p>
</div>
</li>
<li>
<p>Use the following command to create the new buckets for <code>datacenter DC1</code> and <code>datacenter DC2</code>:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd crush add-bucket DC1 datacenter
ceph osd crush add-bucket DC2 datacenter</code></pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Move the DC1 and DC2 datacenter buckets under the root default bucket.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd crush move DC1 root=default
ceph osd crush move DC2 root=default</code></pre>
</div>
</div>
</li>
<li>
<p>Move each of the hosts and their osds under each datacenter.</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd crush move ceph1 datacenter=DC1
ceph osd crush move ceph2 datacenter=DC1
ceph osd crush move ceph3 datacenter=DC1
ceph osd crush move ceph4 datacenter=DC2
ceph osd crush move ceph5 datacenter=DC2
ceph osd crush move ceph6 datacenter=DC2</code></pre>
</div>
</div>
</li>
<li>
<p>Check the CRUSH map again with the ceph osd tree command to see how ceph is now mapped.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd tree</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>ID   CLASS  WEIGHT   TYPE NAME           STATUS  REWEIGHT  PRI-AFF
-1          0.87900  root default
-16         0.43950      datacenter DC1
-11         0.14650          host ceph1
  2    ssd  0.14650              osd.2       up   1.00000  1.00000
 -3         0.14650          host ceph2
  3    ssd  0.14650              osd.3       up   1.00000  1.00000
-13         0.14650          host ceph3
  4    ssd  0.14650              osd.4       up   1.00000  1.00000
-17         0.43950      datacenter DC2
 -5         0.14650          host ceph4
  0    ssd  0.14650              osd.0       up   1.00000  1.00000
 -9         0.14650          host ceph5
  1    ssd  0.14650              osd.1       up   1.00000  1.00000
 -7         0.14650          host ceph6
  5    ssd  0.14650              osd.5       up   1.00000  1.00000</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Create a CRUSH rule that makes use of this new topology by installing the ceph-base RPM package in order to use the crushtool command:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">dnf -y install ceph-base</code></pre>
</div>
</div>
<div class="paragraph">
<p>To know more about CRUSH ruleset, see
<a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/architecture_guide/the-core-ceph-components#ceph-crush-ruleset_arch">Ceph
CRUSH ruleset.</a></p>
</div>
</div>
</div>
</li>
<li>
<p>Get the compiled CRUSH map from the cluster:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd getcrushmap &gt; /etc/ceph/crushmap.bin</code></pre>
</div>
</div>
</li>
<li>
<p>Decompile the CRUSH map and convert it to a text file in order to be able to edit it:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">crushtool -d /etc/ceph/crushmap.bin -o /etc/ceph/crushmap.txt</code></pre>
</div>
</div>
</li>
<li>
<p>Add the following rule to the CRUSH map by editing the text file /etc/ceph/crushmap.txt at the end of the file.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">vim /etc/ceph/crushmap.txt

...
rule stretch_rule {
        id 1
        type replicated
        min_size 1
        max_size 10
        step take DC1
        step chooseleaf firstn 2 type host
        step emit
        step take DC2
        step chooseleaf firstn 2 type host
        step emit
}

# end crush map</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The rule <code>id</code> has to be unique in our case we only have one more crush rule with
id 0 that is why we are using id 1, if your deployment has more rules created,
please use the next free id.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The CRUSH rule we have declared contains the following information:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Rule name</code>:</p>
<div class="ulist">
<ul>
<li>
<p>Description: A unique whole name for identifying the rule.</p>
</li>
<li>
<p>Value: <code>stretch_rule</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><code>id</code>:</p>
<div class="ulist">
<ul>
<li>
<p>Description: A unique whole number for identifying the rule.</p>
</li>
<li>
<p>Value: <code>1</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><code>type</code>:</p>
<div class="ulist">
<ul>
<li>
<p>Description: Describes a rule for either a storage drive replicated or erasure-coded.</p>
</li>
<li>
<p>Value: <code>replicated</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><code>min_size</code>:</p>
<div class="ulist">
<ul>
<li>
<p>Description: If a pool makes fewer replicas than this number, CRUSH will not select this rule.</p>
</li>
<li>
<p>Value: <code>1</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><code>max_size</code>:</p>
<div class="ulist">
<ul>
<li>
<p>Description: If a pool makes more replicas than this number, CRUSH will not select this rule.</p>
</li>
<li>
<p>Value: <code>10</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><code>step take DC1</code></p>
<div class="ulist">
<ul>
<li>
<p>Description: Takes a bucket name (DC1), and begins iterating down the tree.</p>
</li>
</ul>
</div>
</li>
<li>
<p><code>step chooseleaf firstn 2 type host</code></p>
<div class="ulist">
<ul>
<li>
<p>Description: Selects the number of buckets of the given type, in this case is two different hosts located in DC1.</p>
</li>
</ul>
</div>
</li>
<li>
<p><code>step emit</code></p>
<div class="ulist">
<ul>
<li>
<p>Description: Outputs the current value and empties the stack. Typically used at the end of a rule, but may also be used to pick from different trees in the same rule.</p>
</li>
</ul>
</div>
</li>
<li>
<p><code>step take DC2</code></p>
<div class="ulist">
<ul>
<li>
<p>Description: Takes a bucket name (DC2), and begins iterating down the tree.</p>
</li>
</ul>
</div>
</li>
<li>
<p><code>step chooseleaf firstn 2 type host</code></p>
<div class="ulist">
<ul>
<li>
<p>Description: Selects the number of buckets of the given type, in this case, is two different hosts located in DC2.</p>
</li>
</ul>
</div>
</li>
<li>
<p><code>step emit</code></p>
<div class="ulist">
<ul>
<li>
<p>Description: Outputs the current value and empties the stack. Typically used at the end of a rule, but may also be used to pick from different trees in the same rule.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</li>
<li>
<p>Compile the new CRUSH map from the file /etc/ceph/crushmap.txt and convert it to a binary file called /etc/ceph/crushmap2.bin:</p>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">crushtool -c /etc/ceph/crushmap.txt -o /etc/ceph/crushmap2.bin</code></pre>
</div>
</div>
</li>
<li>
<p>Inject the new crushmap we created back into the cluster:</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd setcrushmap -i /etc/ceph/crushmap2.bin</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>17</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The number 17 is a counter and it will increase (18,19, and so on) depending on the changes you make to the crush map
</td>
</tr>
</table>
</div>
</div>
</div>
</li>
<li>
<p>Verify that the stretched rule created is now available for use.</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph osd crush rule ls</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>replicated_rule
stretch_rule</pre>
</div>
</div>
</div>
</div>
</li>
<li>
<p>Enable stretch cluster mode</p>
<div class="openblock">
<div class="content">
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph mon enable_stretch_mode ceph7 stretch_rule datacenter</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, <code>ceph7</code> is the arbiter node, <code>stretch_rule</code> is the crush rule we created in the previous step and <code>datacenter</code> is the dividing bucket.</p>
</div>
<div class="paragraph">
<p>Verify all our pools are using the <code>stretch_rule</code> CRUSH rule we have created in our Ceph cluster:</p>
</div>
<div class="listingblock execute">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">for pool in $(rados lspools);do echo -n "Pool: ${pool}; ";ceph osd pool get ${pool} crush_rule;done</code></pre>
</div>
</div>
<div class="literalblock">
<div class="title">Example output.</div>
<div class="content">
<pre>Pool: device_health_metrics; crush_rule: stretch_rule
Pool: cephfs.cephfs.meta; crush_rule: stretch_rule
Pool: cephfs.cephfs.data; crush_rule: stretch_rule
Pool: .rgw.root; crush_rule: stretch_rule
Pool: default.rgw.log; crush_rule: stretch_rule
Pool: default.rgw.control; crush_rule: stretch_rule
Pool: default.rgw.meta; crush_rule: stretch_rule
Pool: rbdpool; crush_rule: stretch_rule</pre>
</div>
</div>
<div class="paragraph">
<p>This indicates that a working Red Hat Ceph Storage stretched cluster with  arbiter mode is now available.</p>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Red Hat Data Services">
  </a>
</footer>
<script src="../../_/js/site.js"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
